---
title: "Dirichlet Process Mixtures: Sensitivity to the prior on the precision parameter"
author: 
  - name: Joon-Ho Lee (joonho@berkeley.edu)
date: "March 11, 2020"
output:
  html_document: 
    css: styles.css
    fig_caption: yes
    highlight: haddock
    number_sections: yes
    theme: readable
    toc: yes
    toc_depth: 2
  tufte::tufte_html:
    number_sections: yes
    toc: true
    toc_depth: 2
  pdf_document:
    toc: yes
    toc_depth: '3'
bibliography: bibliography.bib
---

```{r basic_setup, include=FALSE}
### Set working directory
setwd("~/Bayes-deconvolution/posts/02_Modeling choices_03_Dirichlet Process_alpha sensitivity 2")

### Set RMarkdown options
knitr::opts_chunk$set(echo = TRUE, warnings = FALSE, message = FALSE)

### Call libraries
library(tidyverse)
library(cowplot)
library(DPpackage)

# ### Set Stan options
# # options(mc.cores = parallel::detectCores())
# rstan_options(auto_write = TRUE)
# Sys.setenv(LOCAL_CPPFLAGS = '-march=native')

### Theme settings
theme_preset <- 
  theme_bw() + 
  theme(panel.background = element_blank(),
        panel.grid = element_blank(), 
        legend.position = "bottom", 
        legend.direction = "horizontal", 
        legend.title = element_blank())
```


# The sensitivity of the Dirichlet process mixture model to the prior on the precision parameter 


The Dirichlet process (DP) prior involves (1) a base distribution $G_{0}$, the expectation of $G$, and (2) a precision or strength parameter $\alpha_0$ governing the concentration of the prior for $G$ about its mean $G_{0}$. For any partition $A_1, ..., A_M$ on the support of $G_0$, the vector $\{G(A_1), ..., G(A_M)\}$ of probabilities $G(A_m)$ contained in the set $\{A_m, m = 1, ..., M\}$ follows a Dirichlet distribution $\text{Dirichlet}(\alpha_0 \cdot G_0(A_1), ..., \alpha_0 \cdot G_0(A_M))$.^[Such an approach may be termed *semiparametric* as it involves a parametric model at the first level for the observations, but a nonparametric model at the second-level.]   


Original forms of the DP prior assumed $G_0$ to be known and fixed. One problem with a Dirichlet Process when $G_0$ is known is that it assigns a probability of 1 to the space of discrete probablity measures, which means that site-level treatment effects $\{\hat{\tau}_k\}^{K}_{k=1}$ are measured exactly. An alternative is to take the parameters in $G_0$ to be unknown, and to follow a set of parametric distributions, with possibly unknown hyperparameters, resulting in a mixture of Dirichlet Process (MDP) model:


$$
\hat{\tau}_{k} \sim N(\tau_{k}, \hat{se}_{k}^2) \quad \forall k, 
$$
$$
\tau_{k} \sim \text{DP}(G_0, \alpha_0) \quad \forall k,  
$$


where $G_0 = N(\tau, \sigma_{\tau}^{2})$, $\tau \sim N(0, 100)$, and $\sigma_{\tau}^{-2} \sim \text{Gamma}(1, 1)$. 


The precision parameter $\alpha_0$ can be considered as random, having a gamma distribution, $\alpha_0 \sim \text{Gamma}(a, b)$, or fixed at some particular value. But the literature on setting the standard prior on the clustering or precision parameter $\alpha$ in the Dirichlet process is limited. @escobar1995bayesian capture the uncertainty in $\alpha_0$ using a $\text{Gamma}(a, b)$ prior distribution with fixed hyperparameters $a$ and $b$. A general problem is that $a$ and $b$ may have a large impact on the posterior distribution of $\alpha_0$, and hence on the clustering behaviour, expecially under small sample sizes. 


In this document, we vary the hyperparameters of the Gamma prior on $\alpha_0$ and examine the sensitivity of posterior inferences to the prior on the precision parameter $\alpha_0$. We are particularly interested in comparing the prior of $\alpha_0$ with the posterior of $\alpha_0$, in particular the prior and posterior variances, to see whether the data provide information on $\alpha_0$. 



# Simulation

$G$ will be simulated to follow a mixture of two Gaussian homoscedastic components, $0.8N(0, 1) + 0.2N(4, 1)$, that is normalized to have mean 0 and variance 1. Here, we vary two conditions: 

- The informativeness of the data (`rgt`)
    + The $\hat{se}_{k}^2$ have geometric mean $GM(\hat{se}_{k}^2)$. Values of $GM(\hat{se}_{k}^2)$ examined here are 0.10, 0.33, and 1.
    + Large values indicate relatively less information in the data about $\tau_{k}$s than smaller values. 
    + Large within-site variance = small information for the site specific effect


- The hyperparameters $a$ and $b$ of the $\text{Gamma}(a, b)$ prior
    + $\text{Gamma}(4, 4)$: favorable to a more bumpy, multimodal distribution $G$ with the prior expected number of $\tau_{k}$'s are 5
    + $\text{Gamma}(10, 0.1)$: more favorable to smoother $G$ with the prior expected number of $\tau_{k}$'s are about 70.  



```{r, echo=FALSE}
GenerateMdp <- function(n, rgt, rsr){
  
  ### Set parameters 
  data_obj <- list()
  data_obj <- NULL
  delta <- 4    # distance between two mixtures
  ups <- 1      # want components to have equal variance
  eps <- 0.2    # mixing proportion (smaller side)
  
  
  ### Define a normalizing factor `a`
  a <- sqrt((1 - eps) + eps*ups^2 + eps*(1 - eps)*delta^2)
  
  
  ### Simulate a mixture of 2 normals with mean 0 and var 1  with this parameterization
  ind <- runif(n) < (1 - eps)
  
  data_obj$tau_k <- ind*rnorm(n, -eps*delta/a, sqrt(1/a^2)) + 
    (1 - ind)*rnorm(n, (1 - eps)*delta/a, sqrt(ups^2/a^2))
  
  
  ### Simulate true distribution for quantiles etc.
  ind <- runif(n) < (1 - eps)
  
  data_obj$tau_k <- ind*rnorm(n, -eps*delta/a, sqrt(1/a^2)) + 
    (1 - ind)*rnorm(n, (1 - eps)*delta/a, sqrt(ups^2/a^2))
  
  tailp <- c(0.05, 0.1, 0.25, 0.75, 0.9, 0.95)
  
  
  ### Generate within-site variances (sds) for K sites
  sigma2max <- rgt*rsr
  sigma2min <- sigma2max/(rsr^2)
  sigma2 <- exp(seq(from = log(sigma2min), to = log(sigma2max), length = n))
  sd <- sqrt(sigma2)
  
  
  ### Generate data object (observed Y)
  data_obj$tau_k_hat <- rnorm(n, data_obj$tau_k, sd)
  data_obj$se_k2 <- sigma2
  data_obj$priquan <- mdppriquan
  return(data_obj)
}
```


We construct a loop over the informativeness of data (`rgt`) and the two choices for the $\text{Gamma}$ prior:


```{r, eval=FALSE}
### Define a vector of varying values of geometric mean 
rgt_vec <- c(0.10, 0.33, 1.00)

### Define a list of multiple parameter choices for alpha
ab_list <- list(c(4, 4), c(10, 0.1)) 


### Generate a datafrmae for the loop reference
loop_ref <- expand.grid(seq_along(rgt_vec), seq_along(ab_list))
list_collect <- list()

for (i in seq(nrow(loop_ref))){
  
  ## Extract loop elements 
  rgt <- rgt_vec[loop_ref[i, ][[1]]]
  alpha_param <- ab_list[[loop_ref[i, ][[2]]]]
  
  ### Generate sample draws from the mixture model
  set.seed(12345)
  df_G <- GenerateMdp(n = 50, rgt = rgt, rsr = 5)[c(1:3)] %>%
    data.frame() %>%
    rownames_to_column("K") %>%
    mutate_at(.vars = c("K"), .funs = as.numeric)
  
  ### Set MCMC parameters
  state <- NULL    # the current value of the parameters
  nburn <- 4000    # the number of burn-in scans
  nsave <- 4000    # the total number of scans to be saved
  nskip <- 20      # the thinning interval
  ndisplay <- 100  # the number of saved scans to be displayed on screen
  
  mcmc <- list(nburn = nburn, nsave = nsave,
               nskip = nskip, ndisplay = ndisplay)
  
  ### Set prior parameters
  prior <- list(a0 = alpha_param[1],  # alpha0: shape param.
                b0 = alpha_param[2],  # alpha0: rate param.  
                tau1 = 1,   # G0 variance: shape param.
                tau2 = 1,   # G0 variance: rate param. 
                mub = 0, 
                Sb = 100)  # G0 mean: mean 
  
  ### Prepare dataset as a matrix form (with y and sigma2)
  mat_DPmeta <- df_G %>% select(tau_k_hat, se_k2) %>% as.matrix()
  
  ### Estimate the Dirichlet Process model using DPpackage 
  outp <- DPmeta(formula = mat_DPmeta ~ 1,
                 prior = prior, mcmc = mcmc, 
                 state = state, status = TRUE)
  
  ### Save the output
  list_collect[[i]] <- outp
  
} ### End of loop 
```


Then, we collect and tidy up the posterior samples from the estimation:

```{r, eval=FALSE}
### Define a function to tidy up the DPmeta objects
get_posterior_DPmeta <- function(outp = outp, nburn = 4000){
  
  # (1) Tidy up the "theta" output object
  df_theta <- outp$save.state$randsave %>%
    data.frame() %>% 
    dplyr::select(-Prediction) %>%
    slice(c((nburn/2 + 1):nburn))  # throw away burn-ins
  colnames(df_theta) <- paste0("tau_k[", seq(ncol(df_theta)), "]")
  
  # (2) Tidy up the G0 parameters (m, s2), alpha0, and N of clusters outputs
  df_hyperparm <- outp$save.state$thetasave %>%
    data.frame() %>%
    dplyr::select(-tau_k_hat) %>%
    rename(G0_mu = mu, G0_s2 = sigma2, Ncluster = ncluster, alpha0 = alpha) %>%
    dplyr::select(G0_mu, G0_s2, alpha0, Ncluster) %>%
    slice(c((nburn/2 + 1):nburn))  # throw away burn-ins
  
  # Return the resulting object
  cbind.data.frame(df_hyperparm, df_theta)
}

### Collect posterior samples per each simulation setting
meta_list <- list()
for (i in seq(nrow(loop_ref))){
  
  ## Extract loop elements 
  rgt <- rgt_vec[loop_ref[i, ][[1]]]
  alpha_param <- ab_list[[loop_ref[i, ][[2]]]]
  
  ##  Get posterior samples
  meta_list[[i]] <- get_posterior_DPmeta(list_collect[[i]], nburn) %>%
    mutate(rgt = paste0("rgt = ", sprintf("%.2f", rgt)), 
           alpha_prior = paste0("gamma(", alpha_param[1], ", ", 
                                alpha_param[2], ")")) %>%
    dplyr::select(rgt, alpha_prior, everything())
}

df_combine <- bind_rows(meta_list) %>% 
  mutate(rgt = factor(rgt, levels =  paste0("rgt = ", sprintf("%.2f", rgt_vec))), 
         alpha_prior = factor(alpha_prior, 
                              levels = c("gamma(4, 4)", "gamma(10, 0.1)")))
```


```{r, echo=FALSE}
df_combine <- readRDS(file = "DP_alpha prior sensitivity.rds")
```

```{r}
dim(df_combine)
```



# Comparing the prior of $\alpha_0$ with the posterior of $\alpha_0$ 

We draw the samples from the $\text{Gamma}$ priors for the $\alpha_0$:

```{r}
### (1) alpha ~ gamma(4, 4)
set.seed(12345)
gm_prior1 <- rgamma(n = 2000, shape = 4, rate = 4)
mean(gm_prior1); sd(gm_prior1)

### (2) alpha ~ gamma(10, 0.1)
set.seed(12345)
gm_prior2 <- rgamma(n = 2000, shape = 10, rate = 0.1)
mean(gm_prior2); sd(gm_prior2)
```

Next, we compare the prior of $\alpha_0$ with the posterior of $\alpha_0$:

```{r, fig.width=10}
###' Generate a dataframe to plot: Bind gamma prior samples
Prior <- c(rep(gm_prior1, times = 3), rep(gm_prior2, times = 3))
  
df_plot <- df_combine %>%
  dplyr::select(rgt, alpha_prior, alpha0) %>%
  arrange(alpha_prior, rgt) %>%
  rename(Posterior = alpha0) %>%
  cbind.data.frame(Prior) %>%
  gather(key = sample, value = value, Prior, Posterior) %>%
  mutate(sample = factor(sample, levels = c("Prior", "Posterior")))

### Plot
title <- c("Precision parameter: Prior vs. Posterior")
subtitle <- c("by the informativeness of the data (rgt) & alpha prior")
ggplot(data = df_plot, aes(x = value, group = sample, fill = sample)) +
  geom_density(position = "identity", size = 0.1, alpha = 0.3) + 
  geom_vline(aes(xintercept = 0), size = 0.3, color = "red", linetype = "dashed") + 
  labs(title = title, subtitle = subtitle, x = NULL) + theme_preset +
  facet_wrap(alpha_prior ~ rgt, scales = "free") + 
  theme_minimal()
```


# Comparison of posterior samples 

## Hyperparameters $\tau$ and $\sigma_{\tau}$

```{r, fig.width=10}
### Generate a dataframe to plot
df_plot <- df_combine %>%
  dplyr::select(rgt, alpha_prior, G0_mu, G0_s2) %>%
  mutate(G0_s2 = log(G0_s2)) %>%    # Log-transformation for G0_s2
  gather(key = variable, value = value, G0_mu, G0_s2) %>%
  mutate(variable = factor(variable, levels = c("G0_mu", "G0_s2")))

### Plot
title <- c("Posterior densities of hyperparameters")
subtitle <- c("by the informativeness of the data (rgt) & alpha prior")

ggplot(data = df_plot, aes(x = value, group = alpha_prior, fill = alpha_prior)) +
  geom_density(position = "identity", size = 0.1, alpha = 0.3) + 
  geom_vline(aes(xintercept = 0), size = 0.3, color = "red", linetype = "dashed") + 
  labs(title = title, subtitle = subtitle, x = NULL) + theme_preset +
  facet_wrap(variable ~ rgt, scales = "free") + 
  theme_minimal()
```


## The number of clusters

```{r, fig.width=10}
### Generate a dataframe to plot
df_plot <- df_combine %>%
  dplyr::select(rgt, alpha_prior, Ncluster) 

### Plot!
title <- c("Posterior densities of the (assumed) number of clusters")
ggplot(data = df_plot, aes(x = Ncluster, group = rgt, fill = rgt)) +
  geom_histogram(position = "dodge", size = 1, alpha = 0.3) + 
  geom_vline(aes(xintercept = 0), size = 0.3, color = "red", linetype = "dashed") + 
  labs(title = title, subtitle = subtitle) + theme_preset + 
  facet_wrap(~ alpha_prior, scales = "free") + 
  scale_x_continuous(breaks = min(df_plot$Ncluster):max(df_plot$Ncluster))
```


## Empirical distribution function (EDF) estimates for $\tau_{k}$s 

```{r, fig.width=10}
### Generate a dataframe to plot
df_plot <- df_combine %>%
  dplyr::select(rgt, alpha_prior, contains("tau_k[")) %>%
  gather(key = variable, value = value, -rgt, -alpha_prior) %>%
  group_by(rgt, alpha_prior, variable) %>%
  summarise(mean = mean(value, na.rm = TRUE), 
            sd = sd(value, na.rm = TRUE)) %>%
  mutate(id = as.numeric(str_extract(variable, "\\d+"))) %>%
  arrange(rgt, alpha_prior, id) 

### Plot!
p1 <- ggplot(data = df_plot %>% rename(value = mean), 
             aes(x = value, group = alpha_prior, fill = alpha_prior)) +
  geom_density(position = "identity", size = 0.1, alpha = 0.3) + 
  geom_vline(aes(xintercept = 0), size = 0.3, color = "red", linetype = "dashed") + 
  labs(title = "EDF of posterior means", subtitle = subtitle, x = NULL) + theme_preset +
  facet_wrap(. ~ rgt, scales = "free") + 
  theme_minimal()

p2 <- ggplot(data = df_plot %>% rename(value = sd), 
             aes(x = value, group = alpha_prior, fill = alpha_prior)) +
  geom_density(position = "identity", size = 0.1, alpha = 0.3) + 
  geom_vline(aes(xintercept = 0), size = 0.3, color = "red", linetype = "dashed") + 
  labs(title = "EDF of posterior SDs", subtitle = subtitle, x = NULL) + theme_preset +
  facet_wrap(. ~ rgt, scales = "free") + 
  theme_minimal()

plot_grid(p1, p2, nrow = 2, labels = "AUTO")
```


## Site-specific individual effect estimates $\tau_{k}$s

```{r, fig.width=10, fig.height=10}
### Randomly sample 10 sites
set.seed(12345)
rand_var <- paste0("tau_k[", sample(1:50, 10, replace = FALSE), "]")

### Generate a dataframe to plot
df_plot <- df_combine %>%
  dplyr::select(rgt, alpha_prior, rand_var) %>%
  gather(key = variable, value = value, -rgt, -alpha_prior) 

### Plot
title <- "Posterior densities of site-specific estimates"
ggplot(data = df_plot, aes(x = value, group = alpha_prior, fill = alpha_prior)) +
  geom_density(position = "identity", size = 0.1, alpha = 0.3) + 
  geom_vline(aes(xintercept = 0), size = 0.3, color = "red", linetype = "dashed") + 
  labs(title = title, subtitle = subtitle, x = NULL) + 
  facet_wrap(variable ~ rgt, scales = "free") + 
  theme_minimal() + theme(legend.position = "bottom")
```


# References



