---
title: "Modeling Choices #03. Dirichlet Process"
author: 
  - name: Joon-Ho Lee (joonho@berkeley.edu)
date: "February 29, 2020"
output:
  html_document: 
    css: styles.css
    fig_caption: yes
    highlight: haddock
    number_sections: yes
    theme: readable
    toc: yes
    toc_depth: 2
  tufte::tufte_html:
    number_sections: yes
    toc: true
    toc_depth: 2
  pdf_document:
    toc: yes
    toc_depth: '3'
bibliography: bibliography.bib
---

```{r basic_setup, include=FALSE}
### Set working directory
setwd("~/Treatment-effect-heterogeneity/Multisite Trials/docs/02_Modeling choices_03_Dirichlet Process")

### Set RMarkdown options
knitr::opts_chunk$set(echo = TRUE, warnings = FALSE, message = FALSE)

### Call libraries
library(tidyverse)
library(cowplot)
library(hhsim)
library(rstan)
library(bayesplot)
library(DPpackage)
library(bspmma)


### Set Stan options
# options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
Sys.setenv(LOCAL_CPPFLAGS = '-march=native')

### Theme settings
theme_preset <- 
  theme_bw() + 
  theme(panel.background = element_blank(),
        panel.grid = element_blank(), 
        legend.position = "bottom", 
        legend.direction = "horizontal", 
        legend.title = element_blank())
```


# The @rubin1981estimation model with Dirichlet Process shrinkage prior 

[Rewrite this section]

Consider $K$ study sites in which researchers perform the same interventions and measure the same outcomes. Each site, indexed by $k$, estimates a treatment effect $\tau_k$ averaged across individuals in the site. The sites don't report $\{\tau_k\}^{K}_{k=1}$: instead, they report $\{\hat{\tau}_k\}^{K}_{k=1}$. Some of the observed variation in $\{\hat{\tau}_k\}^{K}_{k=1}$ is sampling variation, yet there is likely to be some genuine variation in effects across settings, often defined as $\sigma_{\tau}^{2} = \text{var}(\tau_{k})$. 


@rubin1981estimation considers a case in which the analyst has access to a set of estimated effects $\{\hat{\tau}_k\}^{K}_{k=1}$ and estimates of the associated sampling errors $\{\hat{se}_{\tau_k}\}^{K}_{k=1}$. Rubin specifies a relationship between the observed estimates and the unobserved $\{\tau_k\}^{K}_{k=1}$, and in addition spcifies a relationship between $\{\tau_k\}^{K}_{k=1}$ and the aggregate parameters $(\tau, \sigma_{\tau}^2)$). The @rubin1981estimation model has a hierarchical likelihood in which each site has own treatment effect parameter, $\tau_k$, but these effects are all drawn from a common distribution governed by $(\tau, \sigma_{\tau}^2)$ as follows:

$$
\hat{\tau}_{k} \sim N(\tau_{k}, \hat{se}_{k}^2) \quad \forall k, 
$$
$$
\tau_{k} \sim t_{\nu = 5}(\tau, \sigma_{\tau}^2) \quad \forall k,  
$$

where $\mathbb{E}(\tau_k) = \tau$ and $\text{var}(\tau_k) = \sigma_{\tau}^{2} \cdot \frac{\nu}{\nu - 2}$. A $t$ prior for a normal mean provides a reasonable posterior even if the data and prior disagree. A $t$ prior provides similar shrinkage to a normal prior when the data and prior agree, but provides little shrinkage when the data and prior disagree. By using the $t$ shrinkage prior, we expect to go beyond normality to add heavy tails. 




# Generate simulated data from a mixture of two Gaussian homoscedastic components


$G$ will be simulated to follow a mixture $0.8N(0, 1) + 0.2N(4, 1)$ that is normalized to have mean 0 and variance 1.  

In general, a mixture model assumes the data are generated by the following process: first we sample $z$, and then we sample the observables $\mathrm{y}$ from a distribution which depends on $z$, i.e,

$$
p(z, \mathrm{y}) = p(z)p(\mathrm{y}|z).
$$

In mixture models, $p(z)$ is always a multinomial distribution. $p(\mathrm{y}|x)$ can take a variety of parametric forms, but we assume that it is a Gaussian distribution. We refer to such a model as a **mixture of Gaussians**. It has the following generative process: 

1. With probability of 0.8, choose the first component ($N(0, 1)$), otherwise choose the second component ($N(4, 1)$). 

2. If we chose the first component, then sample $y$ from a Gaussian with mean 0 and standard deviation 1. 

3. If we chose the second component, then sample $y$ from a Gaussian with mean 4 and standard deviation 1. 

In general,  we can compute the probability density function (PDF) over $\mathrm{y}$ by marginalizing out, or summing out, $z$:

$$
\begin{array}{rcl}
p(\mathrm{y}) &=& \displaystyle\sum_{z}{p(z)p(\mathrm{y}|z)} 
\\ &=& \displaystyle\sum_{k = 1}^{K}{\mathrm{Pr}(z=k)p(\mathrm{y}|z = k)}.
\end{array}
$$

The **R** function that follows simulates a dataset conforming to the mixture model. We first generate a random vector indicating components.

```{r}
### Set parameters
tailp <- c(0.05, 0.1, 0.25, 0.75, 0.9, 0.95)  # tail probabilities
delta <- 4    # distance between two mixtures
ups <- 1      # want components to have equal variance
eps <- 0.2    # mixing proportion (smaller side)

### Generate a random vector indicating components 
ind <- runif(20000) < (1 - eps)
prop.table(table(ind))
```


Then we draw samples from the PDF which is a convex combination or weighted avearage of the PDFs of the component distributions. Note, however, that we want the $\mathrm{y}$ to be normalized to have mean 0 and variance 1. We thus first define a normalizing factor `a`, and then generate true PDF of $y$s from the mixture distribution with 20,000 simulated draws. 

```{r}
### Define a normalizing factor `a`
a <- sqrt((1 - eps) + eps*ups^2 + eps*(1 - eps)*delta^2)
a

### Get true PDF of thetas from mixture
quant <- ind*rnorm(20000, -eps*delta/a, sqrt(1/a^2)) + 
  (1 - ind)*rnorm(20000, (1 - eps)*delta/a, sqrt(ups^2/a^2))

### Get true tail quantiles
mdppriquan <- quantile(quant, tailp)

### Check mean, SD, and quantiles of the true PDF
c(mean(quant), sd(quant))
```

```{r}
mdppriquan
```

The `GenerateMdp` function can be defined as follows.

```{r}
### Define a function to generate random draws from the mixture distribution
GenerateMdp <- function(n, rgt, rsr){
  
  ### Set parameters 
  data_obj <- list()
  data_obj <- NULL
  delta <- 4    # distance between two mixtures
  ups <- 1      # want components to have equal variance
  eps <- 0.2    # mixing proportion (smaller side)
  
  ### Define a normalizing factor `a`
  a <- sqrt((1 - eps) + eps*ups^2 + eps*(1 - eps)*delta^2)
  
  ### Simulate a mixture of 2 normals with mean 0 and var 1  with this parameterization
  ind <- runif(n) < (1 - eps)
  data_obj$tau_k <- ind*rnorm(n, -eps*delta/a, sqrt(1/a^2)) + 
    (1 - ind)*rnorm(n, (1 - eps)*delta/a, sqrt(ups^2/a^2))
  
  ### Simulate true distribution for quantiles etc.
  ind <- runif(n) < (1 - eps)
  data_obj$tau_k <- ind*rnorm(n, -eps*delta/a, sqrt(1/a^2)) + 
    (1 - ind)*rnorm(n, (1 - eps)*delta/a, sqrt(ups^2/a^2))
  tailp <- c(0.05, 0.1, 0.25, 0.75, 0.9, 0.95)
  
  ### Generate within-site variances (sds) for K sites
  sigma2max <- rgt*rsr
  sigma2min <- sigma2max/(rsr^2)
  sigma2 <- exp(seq(from = log(sigma2min), to = log(sigma2max), length = n))
  sd <- sqrt(sigma2)
  
  ### Generate data object (observed Y)
  data_obj$tau_k_hat <- rnorm(n, data_obj$tau_k, sd)
  data_obj$se_k2 <- sigma2
  data_obj$priquan <- mdppriquan
  return(data_obj)
}

### Generate sample draws from the mixture model
set.seed(12345)
df_G <- GenerateMdp(n = 50, rgt = 1, rsr = 5)[c(1:3)] %>%
  data.frame() %>%
  rownames_to_column("K") %>%
  mutate_at(.vars = c("K"), .funs = as.numeric)

head(df_G)
```


```{r, echo=FALSE}
write.csv(df_G, file = "df_G.csv")
df_G <- read.csv(file = "df_G.csv")
```


Next, we compare the observed estimates ($Y_k$) to the true distribution of $\theta_k$.

```{r, fig.width=10}
### Define a function to generate plot
plot_compare_true_obs <- function(df_G){
  
  # Compare the true distribution (tau_k) to the observed data (Y)
  df_plot <- df_G %>%
    select(tau_k, tau_k_hat) %>% 
    rename(True = tau_k, Observed = tau_k_hat) %>%
    gather(key = variable, value = value)
  
  p1 <- ggplot(data = df_plot, aes(x = value, group = variable, fill = variable)) +
      geom_density(position = "identity", size = 0.1, alpha = 0.3) + 
      geom_vline(aes(xintercept = 0), size = 0.3, color = "red", linetype = "dashed") + 
      labs(title = bquote("True vs. Observed " ~ tau_k ~" Distributions"), 
           x = expression(tau_k)) + theme_preset

  # Compare point estimates by sigma_k
  p2 <- ggplot(data = df_G %>% mutate(gap = abs(tau_k_hat - tau_k)), 
               aes(x = se_k2, y = gap)) + 
    geom_point(aes(size = se_k2)) + 
    labs(title = bquote("|Gap between True and Observed| vs. " ~ sigma^2), 
         y = bquote("Gap between True and Observed " ~ tau_k), 
         x = expression(sigma^2)) + theme_preset
  
  # Combine the two plots
  p_grid <- plot_grid(p1, p2, labels = "AUTO")
  return(p_grid)
}

plot_compare_true_obs(df_G)
```
  




# Parameter estimation using the `hhsim` package

First, we obtain simulates from the posterior of a Dirichlet Process mixture model using the `hhsim` package. The **Stan** programs from the previous Gaussian and $t_{\nu = 5}$ cases run 4 chains with a burn-in period of 500 iterations followed by 500 additional iterations per chain. Since the `hhsim` package does not use separate chains, we set the number of  MCMC and burn-in iterations to 2,000, respectively, in order for the total iterations to be the same with those of the **Stan** program.    


```{r}
### Set numbers of iterations
nmcmc <- 2000    # number of MCMC iterations per each model fit
nburn <- 2000    # burn-in iterations to discard at each round
nmcmcall <- nmcmc + nburn  

### Extract vectors of observed tau_ks and their SEs
Y <- df_G$tau_k_hat   # a vector of observed tau_ks
sigma2 <- df_G$se_k2  # a vector of known SEs
nDraws <- nmcmcall    # number of MCMC draws
```

```{r, eval=FALSE}
### Estimate the Dirichlet Process model using hhsim package
outp <- GaussianMDP(Y, sigma2, nDraws,
                    numconfig = 3,  # N of clusters
                    m = 0,          # mean of G0     
                    w = 1,          # G0 variance: shape param. 
                    W = 1,          # G0 variance: rate param. 
                    par1 = 4,       # alpha0: shape param.
                    par2 = 4)       # alpha0: rate param.
```


```{r, echo=FALSE}
outp <- readRDS("outp.RDS")
```



```{r}
### Define a function to tidy up the output object
get_posterior_sample <- function(output = outp, nburn = 100, nmcmcall = 600){
  
  # (1) Tidy up the "theta" output object
  df_theta <- outp[["theta"]] %>%
    unlist() %>%
    matrix(nrow = length(outp[["theta"]]), byrow = TRUE) %>%
    data.frame() %>%
    slice(c((nburn + 1):nmcmcall))  # throw away burn-ins
  colnames(df_theta) <- paste0("tau_k[", seq(ncol(df_theta)), "]")
  
  # (2) Tidy up the G0 parameters (m, s2), alpha0, and N of clusters outputs
  df_hyperparm <- data.frame(outp[["m"]], outp[["s2"]], 
                             outp[["alpha"]], outp[["numconfig"]]) %>%
    slice(c((nburn + 1):nmcmcall))  # throw away burn-ins
  colnames(df_hyperparm) <- c("G0_mu", "G0_s2", "alpha0", "Ncluster")
  
  ### Return the resulting object
  cbind.data.frame(df_hyperparm, df_theta)
}

### Save posterior samples
posterior_hhsim <- get_posterior_sample(outp, nburn, nmcmcall) %>%
  mutate(Type = "hhsim") %>% dplyr::select(Type, everything())
```




# Parameter estimation using the `DPpackage` package

Second, we obtain simulates from the posterior of a Dirichlet Process mixture model using the `DPpackage` package. 


```{r, results=FALSE}
### Set initial state (the current value of the parameters)
state <- NULL

### Set MCMC parameters
nburn <- 4000    # the number of burn-in scans
nsave <- 2000    # the total number of scans to be saved
nskip <- 20      # the thinning interval
ndisplay <- 100  # the number of saved scans to be displayed on screen

mcmc <- list(nburn = nburn, nsave = nsave,
             nskip = nskip, ndisplay = ndisplay)

### Set prior parameters
prior <- list(a0 = 4,     # alpha0: shape param.
              b0 = 4,     # alpha0: rate param.  
              tau1 = 1,   # G0 variance: shape param.
              tau2 = 1,   # G0 variance: rate param. 
              mub = 0, 
              Sb = 100)  # G0 mean: mean 

### Prepare dataset
df_temp <- df_G %>% select(tau_k_hat, se_k2) %>% as.matrix()

### Estimate the Dirichlet Process model using DPpackage 
fit <- DPmeta(formula = df_temp ~ 1,
              prior = prior, mcmc = mcmc, 
              state = state, status = TRUE)
```


```{r, fig.width=10}
# Summary with HPD and Credibility intervals
summary(fit)
summary(fit, hpd = FALSE)

# Plot model parameters (to see the plots gradually set ask=TRUE)
plot(fit, ask = FALSE)
plot(fit, ask = FALSE, nfigr = 2, nfigc = 2)
```


```{r}
### (1) Tidy up the "theta" output object
df_theta <- fit$save.state$randsave %>%
  data.frame() %>% 
  dplyr::select(-Prediction)
colnames(df_theta) <- paste0("tau_k[", seq(ncol(df_theta)), "]")

### (2) Tidy up the G0 parameters (m, s2), alpha0, and N of clusters outputs
df_hyperparm <- fit$save.state$thetasave %>%
  data.frame() %>%
  dplyr::select(-tau_k_hat) %>%
  rename(G0_mu = mu, G0_s2 = sigma2, Ncluster = ncluster, alpha0 = alpha) %>%
  dplyr::select(G0_mu, G0_s2, alpha0, Ncluster)

### Return the resulting object
posterior_DPmeta <- cbind.data.frame(df_hyperparm, df_theta) %>% 
  mutate(Type = "DPmeta") %>% dplyr::select(Type, everything())
```



# Parameter estimation using the `bspmma` package

Third, we obtain simulates from the posterior of a Dirichlet Process mixture model using the `bspmma` package. 


```{r}
### Estimate the Dirichlet Process model using bspmma 
fit_bspmma <- dirichlet.c(df_temp, 
                          ncycles = 4000, 
                          M = 5)
```



```{r}
df_theta <- fit_bspmma$chain %>%
  data.frame() %>%
  slice(c((2000 + 1):4000)) %>% # throw away burn-ins
  dplyr::select(-mu, -tau) 
colnames(df_theta) <- paste0("tau_k[", seq(ncol(df_theta)), "]")

df_hyperparm <- fit_bspmma$chain %>%
  data.frame() %>%
  slice(c((2000 + 1):4000)) %>% # throw away burn-ins
  dplyr::select(mu, tau) %>%
  rename(G0_mu = mu, G0_s2 = tau)
  
### Return the resulting object
posterior_bspmma <- cbind.data.frame(df_hyperparm, df_theta) %>% 
  mutate(Type = "bspmma") %>% dplyr::select(Type, everything())
```







# Comparison of posterior samples obtained from `hhsim` and `DPpackage`

Would the `DPpackage` and `hhsim` packages lead to the same estimates? We start from combining the two data frames containing posterior samples:

```{r}
### Combine two dataframes: Stan and hhsim
df_combine <- posterior_hhsim %>%
  bind_rows(posterior_DPmeta) %>% 
  bind_rows(posterior_bspmma) %>%
  mutate(Type = factor(Type, levels = c("hhsim", "DPmeta", "bspmma")))
```


From the results presented below, we conclude that the `DPpackage` and the `hhsim` packages generates more or less similar posterior distributions for (1) hyperparameters ($\tau$ and $\sigma_{\tau}$), (2) empirical distribution function (EDF) estimates for $\tau_{k}$s, and (3) site-specific individual effect estimates $\tau_{k}$s.     


## Hyperparameters $\tau$ and $\sigma_{\tau}$

```{r, fig.width=10}
### Generate a dataframe to plot
df_plot <- df_combine %>%
  dplyr::select(Type, G0_mu, G0_s2, alpha0, Ncluster) %>%
  gather(key = variable, value = value, -Type) %>%
  mutate(variable = factor(variable, levels = c("G0_mu", "G0_s2", "alpha0", "Ncluster")))

### Define a function to generate plot
plot_compare_density <- function(df_plot, title = title){
  ggplot(data = df_plot, aes(x = value, group = Type, fill = Type)) +
    geom_density(position = "identity", size = 0.1, alpha = 0.3) + 
    geom_vline(aes(xintercept = 0), size = 0.3, color = "red", linetype = "dashed") + 
    labs(title = title) + theme_preset
}

plot_compare_density(df_plot, title = "Posterior densities of hyperparameters (Stan vs. hhsim)") + 
  facet_wrap(~ variable, scales = "free")
```


## Empirical distribution function (EDF) estimates for $\tau_{k}$s 

```{r}
df_temp <- df_combine %>%
  dplyr::select(Type, contains("tau_k[")) %>%
  gather(key = variable, value = value, -Type) %>%
  group_by(Type, variable) %>%
  summarise(mean = mean(value, na.rm = TRUE), 
            sd = sd(value, na.rm = TRUE)) %>%
  mutate(id = as.numeric(str_extract(variable, "\\d+"))) %>%
  arrange(id, Type)

head(df_temp, 10)  
```


```{r, fig.width=10}
p1 <- plot_compare_density(df_temp %>% rename(value = mean), title = "EDF of posterior means")
p2 <- plot_compare_density(df_temp %>% rename(value = sd), title = "EDF of posterior SDs")
plot_grid(p1, p2, labels = "AUTO")
```



## Site-specific individual effect estimates $\tau_{k}$s


```{r, fig.width=10, fig.height=7}
### Compare densities: site-specific estiamtes
set.seed(12345)
rand_var <- paste0("tau_k[", sample(1:50, 10, replace = FALSE), "]")

df_plot <- df_combine %>%
  dplyr::select(Type, rand_var) %>%
  gather(key = variable, value = value, -Type)

title <- "Posterior densities of site-specific estimates (Stan vs. hhsim)"
plot_compare_density(df_plot, title = title) + 
  facet_wrap(~ variable, scales = "free")
```


# References



